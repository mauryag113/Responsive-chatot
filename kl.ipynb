{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maury\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\maury\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot Response: Based on the context: What payment methods are accepted? What is the return policy for our products? How can customers track their orders?, here is an answer to your query.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer and model from Hugging Face\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Helper function to get embeddings using BERT\n",
    "def get_embedding(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    # Get the BERT embeddings (we use the last hidden state)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the token embeddings as the sentence embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Step 1: Define documents\n",
    "documents = {\n",
    "    \"doc1\": \"What is the return policy for our products?\",\n",
    "    \"doc2\": \"How can customers track their orders?\",\n",
    "    \"doc3\": \"What payment methods are accepted?\"\n",
    "}\n",
    "\n",
    "# Step 2: Generate embeddings for each document\n",
    "document_embeddings = []\n",
    "metadata = []\n",
    "for doc_id, content in documents.items():\n",
    "    embedding = get_embedding(content)\n",
    "    document_embeddings.append(embedding)\n",
    "    metadata.append({\"id\": doc_id, \"text\": content})\n",
    "\n",
    "# Convert embeddings to a NumPy array for vector search\n",
    "embedding_matrix = np.array(document_embeddings)\n",
    "\n",
    "# Step 3: Querying the system\n",
    "def query_qa_bot(query):\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Compute cosine similarities between the query and document embeddings\n",
    "    similarities = np.dot(embedding_matrix, query_embedding) / (np.linalg.norm(embedding_matrix, axis=1) * np.linalg.norm(query_embedding))\n",
    "    \n",
    "    # Get the indices of the top 3 most similar documents\n",
    "    top_k_indices = similarities.argsort()[-3:][::-1]\n",
    "    \n",
    "    # Retrieve matching contexts\n",
    "    contexts = [metadata[idx][\"text\"] for idx in top_k_indices]\n",
    "    combined_context = \" \".join(contexts)\n",
    "    \n",
    "    # Simulate a response based on the context\n",
    "    response = f\"Based on the context: {combined_context}, here is an answer to your query.\"\n",
    "    return response\n",
    "\n",
    "# Test the QA bot with a user query\n",
    "user_query = \"What payment options do you offer?\"\n",
    "bot_response = query_qa_bot(user_query)\n",
    "print(\"Bot Response:\", bot_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
